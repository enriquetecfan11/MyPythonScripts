{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f6ff43-e425-424c-9dae-f42a1ec7092f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Building Conversational Bot using Langchain\n",
    "\n",
    "* In this notebook we shall utilize the capabilities of langchain (PromptTemplate, LLMChain, LLM interface </br>\n",
    "</br>\n",
    "We shall build a conversational bot and create an interface like ChatGPT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101fbee4-4adc-4248-94d1-44e328e99756",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1582b-ca9f-4480-a89e-76c84bf9bc50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install huggingface_hub\n",
    "#!pip install transformers\n",
    "#!pip install langchain\n",
    "#!pip install chainlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6aa3f7-adad-4997-9d86-17dd9fdfc212",
   "metadata": {},
   "source": [
    "I have already installed these libraries in my environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb339a-be73-4a76-9412-75ef63177c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!chainlit hello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6329b396-47bc-47ce-ad86-8906cc1e2d06",
   "metadata": {},
   "source": [
    "### Importing libraries and access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a969e5-0054-4545-b26b-812923ec2f74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import chainlit as cl\n",
    "from langchain import HuggingFaceHub, PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d2430-1cea-45f3-92a9-2fe1f01cde4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e572beee-4dcb-4093-9fd3-78ff9c075fd7",
   "metadata": {},
   "source": [
    "* The PromptTemplate is one of the elements of LangChain, necessary for building applications based on the Large Language Model. It defines how the model should interpret the userâ€™s questions and in what context it should answer them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6ef582-ecd6-47b1-87e6-1fefa1b24085",
   "metadata": {},
   "source": [
    "### Setting the conversational model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aefe8e-c88f-491f-8894-741b0fc74600",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_id = \"microsoft/DialoGPT-medium\" : conversational models are not currently supported by Langchain\n",
    "#model_id = \"mosaicml/mpt-7b-instruct\"\n",
    "#model_id = \"tiiuae/falcon-7b\"\n",
    "model_id = \"gpt2-medium\"  #355M parameters\n",
    "conv_model = HuggingFaceHub(huggingfacehub_api_token=os.environ['HUGGINGFACEHUB_API_TOKEN'],\n",
    "                            repo_id=model_id,\n",
    "                            model_kwargs={\"temperature\":0.8, \"max_new_tokens\":200}) #0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f4407e-dfe4-4b7e-9b9d-11ea047e3968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful AI assistant that makes stories by completing the query provided by the user \n",
    "\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa89aee-7e5f-4af3-9164-87654529f9ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv_chain = LLMChain(llm=conv_model,\n",
    "                      prompt=prompt,\n",
    "                      verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782ecce6-dc8c-4859-a0b7-bf0bb05b9b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(conv_chain.run(\"Once upon a time in 1947\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534d3cc-3350-4976-8dd0-885cdc195e8c",
   "metadata": {},
   "source": [
    "### Creating chatbot interface with Chainlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61877b3-e524-4c06-9e4f-2917fe1e1020",
   "metadata": {},
   "source": [
    "Chainlit is a python package to create UI for chat interface applications </br>\n",
    "We need to use the decorator from Chainlit fro langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0997546-c92e-44a1-b637-1ecd006f98d5",
   "metadata": {},
   "source": [
    "https://docs.chainlit.io/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ddee6f-cece-4657-91fb-bc9c369ebf3b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@cl.langchain_factory(use_async=False)\n",
    "\n",
    "def factory():\n",
    "    prompt = PromptTemplate(template=template, input_variables=['question'])\n",
    "    conv_chain = LLMChain(llm=conv_model,\n",
    "                          prompt=prompt,\n",
    "                          verbose=True)\n",
    "    \n",
    "    return conv_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10047624-cb09-4104-afcf-092b5869e631",
   "metadata": {},
   "source": [
    "### Using Conversational memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d4a7ff-9edf-4ba2-a756-4bfe72d5a1ab",
   "metadata": {},
   "source": [
    "Conversational memory is how a chatbot can respond to multiple queries in a chat-like manner </br>\n",
    "It enables a coherent conversation, and without it, every query would be treated as an entirely independent input without considering past interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5994a34e-9b4d-4081-8099-87e7c8c73289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5d26ff-98fc-4172-8669-de18f8733725",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_message = \"When I was a child\"\n",
    "while user_message!='bye':\n",
    "    memory.chat_memory.add_user_message(user_message)\n",
    "    resp = conv_chain.run(user_message)\n",
    "    print(\"AI: \",resp)\n",
    "    memory.chat_memory.add_ai_message(resp)\n",
    "    \n",
    "    user_message = input(\"Enter a message or bye to exit!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38400381-7d11-4bb0-91b6-cd80fea7df15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_with_memory = \"\"\"You are a helpful chatbot. You answer questions \n",
    "after some thought and only provides relevant answer\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "New human question: {question}\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt2 = PromptTemplate(template=template_with_memory, input_variables=['chat_history','question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4898316e-4beb-430f-8b3e-94022c22ea2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"history\")\n",
    "\n",
    "model_id = \"gpt2-xl\"\n",
    "conv_model = HuggingFaceHub(huggingfacehub_api_token=os.environ['HUGGINGFACEHUB_API_TOKEN'],\n",
    "repo_id=model_id,model_kwargs={\"temperature\":0.8, \"max_length\":128})\n",
    "\n",
    "conv_chain_with_memory = ConversationChain(llm=conv_model, memory=memory, verbose=True)\n",
    "\n",
    "# I want to stop when the AI says\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903cc70b-4c96-4aaf-ae92-e1807faac6f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(conv_chain_with_memory.predict(input=\"Hi there!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79d0cf-2bac-4828-9490-c531633b8f01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(conv_chain_with_memory.predict(input=\"Yes, I have a Mercedes. Wanna come on drive?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a275d61-9a9f-4226-bf30-0278f68b1ae0",
   "metadata": {},
   "source": [
    "#### Types of Memory\n",
    "\n",
    "1. ConversationBufferMemory: This memory allows for storing of messages and then extracts the messages in a \n",
    "variable\n",
    "2. ConversationBufferWindowMemory: keeps a list of the interactions of the conversation over time. It only uses the last K interactions. Useful for keeping a sliding window of the most recent interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55850faf-0883-40c4-9a06-3404278a5a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
